{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Explaining Machine Learning\n",
    "\n",
    "Interpretability of Machine Learning models has recently become a relevant \n",
    " research direction to more thoroughly address and mitigate the issues of\n",
    " adversarial examples and to better understand the potential flows of the \n",
    " most recent algorithm such as Deep Neural Networks.\n",
    "\n",
    "In this tutorial, we explore different methods that SecML provides to compute \n",
    " *post-hoc* explanations, which consist on analyzing a trained model to \n",
    " understand which components such as features or training prototypes are\n",
    " more relevant during the decision (classification) phase.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/maurapintor/secml_notebooks/blob/HEAD/04_explainability.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr --no-display\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "try:\n",
    "  import secml\n",
    "  import foolbox\n",
    "except ImportError:\n",
    "  %pip install git+https://github.com/pralab/secml\n",
    "  %pip install foolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-based explanations\n",
    "\n",
    "Feature-based explanation methods assign a value to each feature of an input\n",
    " sample depending on how relevant it is towards the classification \n",
    " decision. These relevance values are often called *attributions*.\n",
    "\n",
    "In this tutorial, we are going to test the following *gradient-based* \n",
    " explanation methods:\n",
    "\n",
    " - **Gradient**\n",
    " \n",
    "  > [[baehrens2010explain]](http://www.jmlr.org/papers/volume11/baehrens10a/baehrens10a.pdf)\n",
    "  > D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen,\n",
    "  > K.-R.Muller, \"How to explain individual classification decisions\",\n",
    "  > in: J. Mach. Learn. Res. 11 (2010) 1803-1831\n",
    " \n",
    " - **Gradient * Input**\n",
    " \n",
    "  > [[shrikumar2016not]](https://arxiv.org/pdf/1605.01713)\n",
    "  > A. Shrikumar, P. Greenside, A. Shcherbina, A. Kundaje,\n",
    "  > \"Not just a blackbox: Learning important features through propagating\n",
    "  > activation differences\", 2016 arXiv:1605.01713.\n",
    "\n",
    "  > [[melis2018explaining]](https://arxiv.org/abs/1803.03544)\n",
    "  > M. Melis, D. Maiorca, B. Biggio, G. Giacinto and F. Roli,\n",
    "  > \"Explaining Black-box Android Malware Detection,\" 2018 26th European \n",
    "  > Signal Processing Conference (EUSIPCO), Rome, 2018, pp. 524-528.\n",
    "\n",
    " - **Integrated Gradients**\n",
    " \n",
    "  > [[sundararajan2017axiomatic]](https://arxiv.org/pdf/1703.01365)\n",
    "  > Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. \"Axiomatic Attribution \n",
    "  > for Deep Networks.\" Proceedings of the 34th International Conference on \n",
    "  > Machine Learning, Volume 70, JMLR. org, 2017, pp. 3319-3328."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the classifier\n",
    "\n",
    "First, we load the MNIST dataset and we train an SVM classifier with RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "random_state = 999\n",
    "\n",
    "n_tr = 500\n",
    "n_ts = 500\n",
    "\n",
    "from secml.data.loader import CDataLoaderMNIST\n",
    "loader = CDataLoaderMNIST()\n",
    "tr = loader.load('training', num_samples=n_tr)\n",
    "ts = loader.load('testing', num_samples=n_ts)\n",
    "\n",
    "tr.X /= 255\n",
    "ts.X /= 255\n",
    "\n",
    "from secml.ml.classifiers import CClassifierSVM\n",
    "from secml.ml.kernels import CKernelRBF\n",
    "\n",
    "clf = CClassifierSVM(kernel=CKernelRBF(gamma=1e-2))\n",
    "\n",
    "print(\"Training of classifier...\")\n",
    "clf.fit(tr.X, tr.Y)\n",
    "\n",
    "y_pred = clf.predict(ts.X)\n",
    "\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the explanations\n",
    "\n",
    "The `secml.explanation` package provides different explanation methods\n",
    " as subclasses of `CExplainer`. Each explainer requires as input a \n",
    " trained classifier.\n",
    "\n",
    "To compute the explanation on a sample, the `.explain()` method should be used.\n",
    "For *gradient-based* methods, the label `y` of the class wrt the explanation\n",
    " should be computed is required.\n",
    "\n",
    "The `.explain()` method will return the relevance value associated to each \n",
    " feature of the input sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.explanation import \\\n",
    "    CExplainerGradient, CExplainerGradientInput, CExplainerIntegratedGradients\n",
    "\n",
    "explainers = (\n",
    "   {\n",
    "    \"exp\": CExplainerGradient(clf),\n",
    "    \"label\": \"gradient\"\n",
    "   },\n",
    "   {\"exp\": CExplainerGradientInput(clf),\n",
    "   \"label\": \"gradient * input\"\n",
    "   },\n",
    "   {\n",
    "    \"exp\": CExplainerIntegratedGradients(clf),\n",
    "    \"label\": \"integrated gradients\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 123  # test sample on which explanations should be computed\n",
    "x, y = ts[i, :].X, ts[i, :].Y\n",
    "\n",
    "print(\"Explanations for sample {:} (true class: {:})\".format(i, y.item()))\n",
    "\n",
    "from secml.array import CArray\n",
    "\n",
    "for expl in explainers:\n",
    "    \n",
    "    print(\"Computing explanations using '{:}'...\".format(expl['label']))\n",
    "    \n",
    "    # compute explanations (attributions) wrt each class\n",
    "    attr = CArray.empty(shape=(tr.num_classes, x.size))\n",
    "    for c in tr.classes:\n",
    "        attr_c = expl['exp'].explain(x, y=c)\n",
    "        attr[c, :] = attr_c\n",
    "\n",
    "    expl['attr'] = attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "We now visualize the explanations computed using the different methods, in rows.\n",
    "In columns, we show the explanations wrt each different class.\n",
    "\n",
    "Above the original tested sample, its true class label is shown.\n",
    "\n",
    "Red (blue) pixels denote positive (negative) relevance of the corresponding\n",
    " feature wrt the specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.figure import CFigure\n",
    "# Only required for visualization in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "fig = CFigure(height=4.5, width=14, fontsize=13)\n",
    "        \n",
    "for i, expl in enumerate(explainers):\n",
    "    \n",
    "    sp_idx = i * (tr.num_classes+1)\n",
    "    \n",
    "    # Original image\n",
    "    fig.subplot(len(explainers), tr.num_classes+1, sp_idx+1)\n",
    "    fig.sp.imshow(x.reshape((tr.header.img_h, tr.header.img_w)), cmap='gray')\n",
    "    \n",
    "    if i == 0:  # For the first row only\n",
    "        fig.sp.title(\"Origin y: {:}\".format(y.item()))\n",
    "        \n",
    "    fig.sp.ylabel(expl['label'])  # Label of the explainer\n",
    "    \n",
    "    fig.sp.yticks([])\n",
    "    fig.sp.xticks([])\n",
    "    \n",
    "    # Threshold to plot positive and negative relevance values symmetrically\n",
    "    th = max(abs(expl['attr'].min()), abs(expl['attr'].max()))\n",
    "    \n",
    "    # Plot explanations\n",
    "    for c in tr.classes:\n",
    "    \n",
    "        fig.subplot(len(explainers), tr.num_classes+1, sp_idx+2+c)\n",
    "        fig.sp.imshow(expl['attr'][c, :].reshape((tr.header.img_h, tr.header.img_w)),\n",
    "                      cmap='seismic', vmin=-1*th, vmax=th)\n",
    "        \n",
    "        fig.sp.yticks([])\n",
    "        fig.sp.xticks([])\n",
    "        \n",
    "        if i == 0:  # For the first row only\n",
    "            fig.sp.title(\"c: {:}\".format(c))\n",
    "\n",
    "fig.title(\"Explanations\", x=0.55)\n",
    "fig.tight_layout(rect=[0, 0.003, 1, 0.94])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both **gradient\\*input** and **integrated gradients** methods we can \n",
    " observe a well defined area of positive (red) relevance for the explanation \n",
    " computed wrt the digit 6. This is expected as the true class of the tested \n",
    " sample is in fact 6. Moreover, a non-zero relevance value is mainly assigned \n",
    " to the features which are present in the tested sample, which is an expected \n",
    " behavior of these explanation methods.\n",
    " \n",
    "Conversely, the **gradient** method assigns relevance to a wider area of the \n",
    " image, even external to the actual digit. This leads to explanations which\n",
    " are in many cases difficult to interpret. For this reason, more advanced\n",
    " explanation methods are often favored."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Compute the explanations of adversarial examples. \n",
    "Use the PGD L-inf attack, with eps=0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.adv.attacks.evasion import CFoolboxPGDLinf\n",
    "\n",
    "y_target  = 2\n",
    "attack = CFoolboxPGDLinf(clf, y_target=y_target, \n",
    "                         lb=0.0, ub=1.0, steps=30, \n",
    "                         epsilons=0.3, abs_stepsize=0.01, \n",
    "                         random_start=False)\n",
    "\n",
    "_, _, adv_ds, _ = attack.run(x, y)\n",
    "\n",
    "pred = clf.predict(adv_ds.X).item()\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for expl in explainers:\n",
    "    \n",
    "    print(\"Computing explanations using '{:}'...\".format(expl['label']))\n",
    "    \n",
    "    # compute explanations (attributions) wrt each class\n",
    "    attr = CArray.empty(shape=(tr.num_classes, x.size))\n",
    "    for c in tr.classes:\n",
    "        attr_c = expl['exp'].explain(adv_ds.X, y=c)\n",
    "        attr[c, :] = attr_c\n",
    "\n",
    "    expl['attr'] = attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = CFigure(height=4.5, width=14, fontsize=13)\n",
    "        \n",
    "for i, expl in enumerate(explainers):\n",
    "    \n",
    "    sp_idx = i * (tr.num_classes+1)\n",
    "    \n",
    "    # Original image\n",
    "    fig.subplot(len(explainers), tr.num_classes+1, sp_idx+1)\n",
    "    fig.sp.imshow(adv_ds.X.reshape((tr.header.img_h, tr.header.img_w)), cmap='gray')\n",
    "    \n",
    "    if i == 0:  # For the first row only\n",
    "        fig.sp.title(\"Orig.: {:} Pred.:{}\".format(y.item(), pred.item()))\n",
    "        \n",
    "    fig.sp.ylabel(expl['label'])  # Label of the explainer\n",
    "    \n",
    "    fig.sp.yticks([])\n",
    "    fig.sp.xticks([])\n",
    "    \n",
    "    # Threshold to plot positive and negative relevance values symmetrically\n",
    "    th = max(abs(expl['attr'].min()), abs(expl['attr'].max()))\n",
    "    \n",
    "    # Plot explanations\n",
    "    for c in tr.classes:\n",
    "    \n",
    "        fig.subplot(len(explainers), tr.num_classes+1, sp_idx+2+c)\n",
    "        fig.sp.imshow(expl['attr'][c, :].reshape((tr.header.img_h, tr.header.img_w)),\n",
    "                      cmap='seismic', vmin=-1*th, vmax=th)\n",
    "        \n",
    "        fig.sp.yticks([])\n",
    "        fig.sp.xticks([])\n",
    "        \n",
    "        if i == 0:  # For the first row only\n",
    "            fig.sp.title(\"c: {:}\".format(c))\n",
    "\n",
    "fig.title(\"Explanations\", x=0.55)\n",
    "fig.tight_layout(rect=[0, 0.003, 1, 0.94])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype-based explanation\n",
    "\n",
    "Prototype-based explanation methods select specifc samples from the training \n",
    "dataset to explain the behavior of machine learning models.\n",
    "\n",
    "In this tutorial, we are going to test the explanation method proposed in:\n",
    "\n",
    "  > [[koh2017understanding]](https://arxiv.org/pdf/1703.04730)\n",
    "  > Koh, Pang Wei, and Percy Liang, \"Understanding black-box predictions\n",
    "  > via influence functions\", in: Proceedings of the 34th International\n",
    "  > Conference on Machine Learning-Volume 70. JMLR. org, 2017."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier\n",
    "\n",
    "As our implementation of the prototype-based explanation methods currently \n",
    " only supports binary classifiers, we load the 2-classes MNIST59 dataset and \n",
    " then we train a SVM classifier with RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tr = 200\n",
    "n_ts = 500\n",
    "\n",
    "digits = (5, 9)\n",
    "\n",
    "loader = CDataLoaderMNIST()\n",
    "tr = loader.load('training', digits=digits, num_samples=n_tr)\n",
    "ts = loader.load('testing', digits=digits, num_samples=n_ts)\n",
    "\n",
    "tr.X /= 255\n",
    "ts.X /= 255\n",
    "\n",
    "clf = CClassifierSVM(kernel=CKernelRBF(gamma=1e-2))\n",
    "\n",
    "print(\"Training of classifier...\")\n",
    "clf.fit(tr.X, tr.Y)\n",
    "\n",
    "y_pred = clf.predict(ts.X)\n",
    "\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the influential training prototypes\n",
    "\n",
    "The `CExplainerInfluenceFunctions` class provides the influence functions \n",
    " prototype-based method described previously. It requires as input the \n",
    " classifier to explain and its training set.\n",
    "It also requires the identifier of the loss used to train the classier. In the \n",
    " case of SVM, it is the `hinge` loss.\n",
    "\n",
    "To compute the influence of each training sample wrt the test samples, \n",
    " the `.explain()` method should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from secml.explanation import CExplainerInfluenceFunctions\n",
    "\n",
    "explanation = CExplainerInfluenceFunctions(clf, tr, outer_loss_idx=\"hinge\")\n",
    "\n",
    "print(\"Computing influence of each training prototype on test samples...\")\n",
    "\n",
    "# compute the explanations\n",
    "infl = explanation.explain(ts.X, ts.Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "We now visualize, wrt each class, the 3 most influential training prototypes \n",
    " for two different test samples. Above each training sample, the influence \n",
    " value is shown.\n",
    " \n",
    "In addition, above the original tested samples, the true class label is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = CFigure(height=3.5, width=9, fontsize=13)\n",
    "    \n",
    "n_xc = 3  # number of tr prototypes to plot per class\n",
    "\n",
    "ts_list = (50, 100)  # test samples to evaluate\n",
    "\n",
    "infl_argsort = infl.argsort(axis=1)  # sort influence values\n",
    "        \n",
    "for i, ts_idx in enumerate(ts_list):\n",
    "    \n",
    "    sp_idx = i * (n_xc*tr.num_classes+1)\n",
    "    \n",
    "    x, y = ts[ts_idx, :].X, ts[ts_idx, :].Y\n",
    "\n",
    "    pred = clf.predict(x)\n",
    "    # original image\n",
    "    fig.subplot(len(ts_list), n_xc*tr.num_classes+1, sp_idx+1)\n",
    "    fig.sp.imshow(x.reshape((tr.header.img_h, tr.header.img_w)), cmap='gray')\n",
    "    \n",
    "    fig.sp.title(\"Orig: {:} Pred{:}\".format(ts.header.y_original[y.item()], ts.header.y_original[pred.item()]))\n",
    "    \n",
    "    fig.sp.yticks([])\n",
    "    fig.sp.xticks([])\n",
    "    \n",
    "    tr_top = infl_argsort[ts_idx, :n_xc]\n",
    "    tr_top = tr_top.append(infl_argsort[ts_idx, -n_xc:])\n",
    "    \n",
    "    # plot top influential training prototypes\n",
    "    for j, tr_idx in enumerate(tr_top[::-1]):  # sort highest first\n",
    "        fig.subplot(len(ts_list), n_xc*tr.num_classes+1, sp_idx+2+j)\n",
    "        fig.sp.imshow(tr.X[tr_idx, :].reshape((tr.header.img_h, tr.header.img_w)), cmap='gray')  \n",
    "    \n",
    "        fig.sp.title(\"{:.2f}\".format(infl[ts_idx, tr_idx].item()))\n",
    "    \n",
    "        fig.sp.yticks([])\n",
    "        fig.sp.xticks([])\n",
    "\n",
    "fig.title(\"Top influential training prototypes\", x=0.57)\n",
    "fig.tight_layout(rect=[0, 0.003, 1, 0.92])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the tested samples we can observe a direct correspondence between \n",
    " the most influencial training prototypes and their true class. Specifically, \n",
    " the samples having highest (lowest) influence values are (are not) from \n",
    " the same true class of the tested samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Visualize the prototypes for a poisoned classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.data.splitter import CTrainTestSplit\n",
    "n_val = 100\n",
    "n_tr = 100\n",
    "splitter = CTrainTestSplit(\n",
    "    train_size=n_tr, test_size=n_val, random_state=random_state)\n",
    "tr_, val_ = splitter.split(tr)\n",
    "\n",
    "lb, ub = 0.0, 1.0\n",
    "\n",
    "# the parameters should be chosen depending on the optimization problem\n",
    "solver_params = {\n",
    "    'eta': 0.1,\n",
    "    'max_iter': 30,\n",
    "    'eps': 1e-6\n",
    "}\n",
    "\n",
    "from secml.adv.attacks import CAttackPoisoningSVM\n",
    "\n",
    "attack = CAttackPoisoningSVM(classifier=clf,\n",
    "                             training_data=tr_,\n",
    "                             val=val_,\n",
    "                             lb=lb, ub=ub,\n",
    "                             solver_params=solver_params,\n",
    "                             random_seed=random_state, solver_type='pgd')\n",
    "\n",
    "n_poisoning_points = 50  # number of poisoning points to generate\n",
    "attack.n_points = n_poisoning_points\n",
    "\n",
    "# run the poisoning attack\n",
    "pois_y_pred, pois_scores, pois_ds, f_opt = attack.run(ts.X, ts.Y)\n",
    "\n",
    "# evaluate the accuracy of the original classifier\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "# evaluate the accuracy after the poisoning attack\n",
    "pois_acc = metric.performance_score(y_true=ts.Y, y_pred=pois_y_pred)\n",
    "\n",
    "print(\"Original accuracy on test set: {:.2%}\".format(acc))\n",
    "print(\"Accuracy after attack on test set: {:.2%}\".format(pois_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoned_ds = tr_.append(pois_ds)\n",
    "poisoned_clf = clf.deepcopy()\n",
    "poisoned_clf.fit(poisoned_ds.X, poisoned_ds.Y)\n",
    "\n",
    "explanation = CExplainerInfluenceFunctions(\n",
    "    poisoned_clf, poisoned_ds, outer_loss_idx='hinge')  # SVM loss is 'hinge'\n",
    "\n",
    "print(\"Computing influence of each training prototype on test samples...\")\n",
    "\n",
    "infl = explanation.explain(ts.X, ts.Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = CFigure(height=3.5, width=9, fontsize=13)\n",
    "    \n",
    "n_xc = 3  # number of tr prototypes to plot per class\n",
    "\n",
    "ts_list = (50, 100)  # test samples to evaluate\n",
    "\n",
    "infl_argsort = infl.argsort(axis=1)  # sort influence values\n",
    "        \n",
    "for i, ts_idx in enumerate(ts_list):\n",
    "    \n",
    "    sp_idx = i * (n_xc*tr.num_classes+1)\n",
    "    \n",
    "    x, y = ts[ts_idx, :].X, ts[ts_idx, :].Y\n",
    "    pred = clf.predict(x)\n",
    "\n",
    "    # original image\n",
    "    fig.subplot(len(ts_list), n_xc*tr.num_classes+1, sp_idx+1)\n",
    "    fig.sp.imshow(x.reshape((tr.header.img_h, tr.header.img_w)), cmap='gray')\n",
    "    \n",
    "    fig.sp.title(\"Orig: {:} Pred: {:}\".format(ts.header.y_original[y.item()], ts.header.y_original[pred.item()]))\n",
    "\n",
    "    \n",
    "    fig.sp.yticks([])\n",
    "    fig.sp.xticks([])\n",
    "    \n",
    "    tr_top = infl_argsort[ts_idx, :n_xc]\n",
    "    tr_top = tr_top.append(infl_argsort[ts_idx, -n_xc:])\n",
    "    \n",
    "    # plot top influential training prototypes\n",
    "    for j, tr_idx in enumerate(tr_top[::-1]):  # sort highest first\n",
    "        fig.subplot(len(ts_list), n_xc*tr.num_classes+1, sp_idx+2+j)\n",
    "        fig.sp.imshow(tr.X[tr_idx, :].reshape((tr.header.img_h, tr.header.img_w)), cmap='gray')  \n",
    "    \n",
    "        fig.sp.title(\"{:.5f}\".format(infl[ts_idx, tr_idx].item()))\n",
    "    \n",
    "        fig.sp.yticks([])\n",
    "        fig.sp.xticks([])\n",
    "\n",
    "fig.title(\"Top influential training prototypes\", x=0.57)\n",
    "fig.tight_layout(rect=[0, 0.003, 1, 0.92])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6829dbcfe73f7e6ba320fd39e7c4bddd23e92d1a15475ecfb0305a1647487c5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
