{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Poisoning Attacks against Machine Learning models\n",
    "\n",
    "In this tutorial we will experiment with **adversarial poisoning attacks** against a Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel.\n",
    "\n",
    "Poisoning attacks are performed at *training time* by injecting *carefully-crafted samples* that alter the classifier decision function so that its behavior at *testing time* is modified.\n",
    "\n",
    "As in the previous tutorials, we will first create and train the classifier, evaluating its performance in the standard scenario, *i.e.*, not under attack, by using the standard accuracy metric.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/maurapintor/secml_notebooks/blob/unsolved/03_poisoning_attacks.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr --no-display\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "try:\n",
    "    import secml\n",
    "    import foolbox\n",
    "except ImportError:\n",
    "    %pip install secml[foolbox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "random_state = 999\n",
    "\n",
    "n_features = 2  # number of features\n",
    "n_samples = 1000  # number of samples\n",
    "centers = [[-1, -1], [+1, +1]]  # centers of the clusters\n",
    "cluster_std = 0.9  # standard deviation of the clusters\n",
    "\n",
    "from secml.data.loader import CDLRandomBlobs\n",
    "\n",
    "dataset = CDLRandomBlobs(n_features=n_features,\n",
    "                         centers=centers,\n",
    "                         cluster_std=cluster_std,\n",
    "                         n_samples=n_samples,\n",
    "                         random_state=random_state).load()\n",
    "\n",
    "n_tr = 100  # number of training set samples\n",
    "n_val = 100  # number of validation set samples\n",
    "n_ts = 100  # number of test set samples\n",
    "\n",
    "# split in training, validation and test\n",
    "from secml.data.splitter import CTrainTestSplit\n",
    "\n",
    "splitter = CTrainTestSplit(\n",
    "    train_size=n_tr + n_val, test_size=n_ts, random_state=random_state)\n",
    "tr_val, ts = splitter.split(dataset)\n",
    "splitter = CTrainTestSplit(\n",
    "    train_size=n_tr, test_size=n_val, random_state=random_state)\n",
    "tr, val = splitter.split(dataset)\n",
    "\n",
    "# normalize the data\n",
    "from secml.ml.features import CNormalizerMinMax\n",
    "\n",
    "nmz = CNormalizerMinMax()\n",
    "tr.X = nmz.fit_transform(tr.X)\n",
    "val.X = nmz.transform(val.X)\n",
    "ts.X = nmz.transform(ts.X)\n",
    "\n",
    "# metric to use for training and performance evaluation\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "# creation of the multiclass classifier with RBF kernel\n",
    "from secml.ml.classifiers import CClassifierSVM\n",
    "from secml.ml.kernels import CKernelRBF\n",
    "\n",
    "clean_clf = CClassifierSVM(kernel=CKernelRBF(gamma=10), C=1)\n",
    "\n",
    "# we can now fit the classifier\n",
    "clean_clf.fit(tr.X, tr.Y)\n",
    "print(\"Training of classifier complete!\")\n",
    "\n",
    "# compute predictions on a test set\n",
    "y_pred = clean_clf.predict(ts.X)\n",
    "\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generation of Poisoning Samples\n",
    "\n",
    "We are going to generate our attack against the SVM classifier using the **gradient-based** algorithm for generating poisoning attacks proposed in the following papers:\n",
    " \n",
    "  > [[biggio12-icml]](https://arxiv.org/abs/1206.6389)\n",
    "  > Biggio, B., Nelson, B. and Laskov, P., 2012. Poisoning attacks against \n",
    "  > support vector machines. In ICML 2012 (Test of Time Award).\n",
    "\n",
    "  > [[biggio15-icml]](https://arxiv.org/abs/1804.07933)\n",
    "  > Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C. and Roli, F., 2015. \n",
    "  > Is feature selection secure against training data poisoning?. In ICML 2015.\n",
    "\n",
    "  > [[demontis19-usenix]](\n",
    "  > https://www.usenix.org/conference/usenixsecurity19/presentation/demontis)\n",
    "  > Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., \n",
    "  > Nita-Rotaru, C. and Roli, F., 2019. Why Do Adversarial Attacks Transfer? \n",
    "  > Explaining Transferability of Evasion and Poisoning Attacks. In 28th Usenix \n",
    "  > Security Symposium, Santa Clara, California, USA.\n",
    "\n",
    "To compute a poisoning point, we have to solve a bi-level optimization problem, namely:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\max _{\\mathbf{x}_c} & \\mathcal{A}\\left(\\mathcal{D}_{\\text {val }}, \\mathbf{w}^{\\star}\\right)=\\sum_{j=1}^m \\ell\\left(y_j, \\mathbf{x}_j, \\mathbf{w}^{\\star}\\right) \\\\\n",
    "\\text { s.t. } & \\mathbf{w}^{\\star} \\in \\underset{\\mathbf{w}}{\\arg \\min } \\quad \\mathcal{L}\\left(\\mathcal{D}_{\\text {tr }} \\cup\\left(\\mathbf{x}_c, y\\right), \\mathbf{w}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{x_c}$ is the poisoning point, $\\mathcal{A}$ is the attacker objective function, $\\mathcal{L}$ is the classifier training function.\n",
    "Moreover, $D_{tr}$ is the training dataset and $D_{val}$ is the validation dataset.\n",
    "The former problem, along with the poisoning point $\\mathbf{x}_c$ is used to train the classifier on the poisoned data, while the latter is used to evaluate the performance on the untainted data.\n",
    "\n",
    "The former equation depends on the classifier weights, which in turns, depends on the poisoning point.\n",
    "\n",
    "This attack is implemented in SecML by different subclasses of the `CAttackPoisoning`.\n",
    "For the purpose of attacking a SVM classifier we use the `CAttackPoisoningSVM` class.\n",
    "\n",
    "As done for the evasion attacks, let's specify the parameters first. We set the bounds of the attack space to the known feature space given by validation dataset.\n",
    "Lastly, we chose the solver parameters for this specific optimization problem.\n",
    "\n",
    "Let's start visualizing the objective function considering a single poisoning point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lb, ub = val.X.min(), val.X.max()  # bounds of the attack space\n",
    "\n",
    "# the parameters should be chosen depending on the optimization problem\n",
    "solver_params = {\n",
    "    'eta': 0.05,\n",
    "    'eta_min': 0.05,\n",
    "    'eta_max': None,\n",
    "    'max_iter': 500,\n",
    "    'eps': 1e-6\n",
    "}\n",
    "\n",
    "from secml.adv.attacks import CAttackPoisoningSVM\n",
    "\n",
    "# TODO write your code here\n",
    "# \n",
    "# create the poisoning attack\n",
    "\n",
    "# TODO write your code here\n",
    "# \n",
    "# chose and set the initial poisoning sample features and label\n",
    "\n",
    "\n",
    "print(\"Initial poisoning sample features: {:}\".format(xc.ravel()))\n",
    "print(\"Initial poisoning sample label: {:}\".format(yc.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from secml.figure import CFigure\n",
    "\n",
    "fig = CFigure(4, 5)\n",
    "grid_limits = [(lb - 0.1, ub + 0.1),\n",
    "               (lb - 0.1, ub + 0.1)]\n",
    "\n",
    "\n",
    "# TODO write your code here\n",
    "# \n",
    "# plot the original dataset\n",
    "# highlight the initial poisoning sample showing it as a star\n",
    "\n",
    "fig.sp.title('Attacker objective')\n",
    "# TODO write your code here\n",
    "# \n",
    "# Plot the attack objective function\n",
    "\n",
    "# plot the box constraint\n",
    "from secml.optim.constraints import CConstraintBox\n",
    "\n",
    "box = CConstraintBox(lb=lb, ub=ub)\n",
    "fig.sp.plot_constraint(box, grid_limits=grid_limits,\n",
    "                       n_grid_points=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we set the desired number of poisoning points to generate, 20 in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "n_poisoning_points = n_samples  # number of poisoning points to generate\n",
    "attack.n_points = n_poisoning_points\n",
    "\n",
    "# TODO write your code here\n",
    "# \n",
    "# run the poisoning attack\n",
    "# this computes performances on the testing set \n",
    "# already (on a poisoned copy of the classifier)\n",
    "\n",
    "# # optionally this can be done outside the attack\n",
    "# poisoning_dataset = tr.append(pois_ds)\n",
    "# poisoned_clf = clean_clf.deepcopy()\n",
    "# poisoned_clf.fit(poisoning_dataset.X, poisoning_dataset.Y)\n",
    "# pois_y_pred = poisoned_clf.predict(ts.X)\n",
    "\n",
    "# TODO write your code here\n",
    "# \n",
    "# evaluate the accuracy of the original classifier\n",
    "# evaluate the accuracy after the poisoning attack\n",
    "\n",
    "print(\"Original accuracy on test set: {:.2%}\".format(acc))\n",
    "print(\"Accuracy after attack on test set: {:.2%}\".format(pois_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that the classifiers has been successfully attacked.\n",
    "To increase the attack power, more poisoning points can be crafted, at the expense of a much slower optimization process.\n",
    "\n",
    "Let's now visualize the attack on a 2D plane.\n",
    "We need to train a copy of the original classifier on the join between the training set and the poisoning points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO write your code here\n",
    "# \n",
    "# train the poisoned classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# define common bounds for the subplots\n",
    "min_limit = min(pois_tr.X.min(), ts.X.min())\n",
    "max_limit = max(pois_tr.X.max(), ts.X.max())\n",
    "grid_limits = [[min_limit, max_limit], [min_limit, max_limit]]\n",
    "\n",
    "fig = CFigure(10, 10)\n",
    "\n",
    "# let's plot:\n",
    "#   1. the clean training data and the clean classifier decision regions\n",
    "#   2. the poisoning training data and the poisoned classifier decision regions\n",
    "#   3. the clean testing data and the clean classifier decision regions\n",
    "#   4. the clean testing data and the poisoned classifier decision regions\n",
    "\n",
    "fig.subplot(2, 2, 1)\n",
    "fig.sp.title(\"Original classifier (training set)\")\n",
    "# TODO write your code here\n",
    "# \n",
    "# plot decision regions of the original classifier and the training set\n",
    "fig.sp.grid(False)\n",
    "\n",
    "fig.subplot(2, 2, 2)\n",
    "fig.sp.title(\"Poisoned classifier (poisoning set)\")\n",
    "# TODO write your code here\n",
    "# \n",
    "# plot decision regions of the poisoned classifier and the training set (including poisoning points)\n",
    "\n",
    "fig.sp.grid(False)\n",
    "\n",
    "fig.subplot(2, 2, 3)\n",
    "fig.sp.title(\"Original classifier (testing set)\")\n",
    "# TODO write your code here\n",
    "# \n",
    "# plot decision regions of the original classifier and the testing set\n",
    "fig.sp.grid(False)\n",
    "\n",
    "fig.subplot(2, 2, 4)\n",
    "fig.sp.title(\"Poisoned classifier (testing set)\")\n",
    "# TODO write your code here\n",
    "# \n",
    "# plot decision regions of the poisoning classifier and the testing set\n",
    "fig.sp.grid(False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see how the SVM classifier decision functions *changes* after injecting the adversarial poisoning points (blue and red stars).\n",
    " \n",
    "For more details about poisoning adversarial attacks, refer to:\n",
    "\n",
    "  > [[biggio18-pr]](https://arxiv.org/abs/1712.03141)\n",
    "  > Biggio, B. and Roli, F., 2018. Wild patterns: Ten years after the rise of \n",
    "  > adversarial machine learning. In Pattern Recognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Performance of classifier for more poisoning samples\n",
    "\n",
    "Instead of hard-coding the number of points, we can compute poisoning for different fractions of points, as we did for evasion attacks.\n",
    "After each poisoning experiment, we train a classifier with both clean and poisoned samples and we compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from secml.array import CArray\n",
    "\n",
    "n_poisoning_samples = CArray([1, 10, 20, 75, 100])\n",
    "\n",
    "poisoned_clfs = []\n",
    "test_accuracies = []\n",
    "\n",
    "# TODO write your code here\n",
    "# \n",
    "# write a for loop that tests the effect of increasing poisoning samples\n",
    "# keep track of the accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(test_accuracies)\n",
    "fig = CFigure()\n",
    "fig.sp.plot(n_poisoning_samples, test_accuracies)\n",
    "fig.sp.xlabel('Poison samples')\n",
    "fig.sp.xticks(n_poisoning_samples)\n",
    "fig.sp.ylabel('Test accuracy')\n",
    "fig.sp.ylim([0,1])\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating robustness of poisoned models to evasion attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.adv.seceval import CSecEval\n",
    "from secml.adv.attacks import CFoolboxPGDL2\n",
    "\n",
    "steps = 100\n",
    "abs_stepsize = 0.05\n",
    "epsilons = CArray.linspace(0, 0.2, 5)\n",
    "\n",
    "# NOTE this is considered cheating!!!\n",
    "# we have to do it for bypassing foolbox's check on lb and ub\n",
    "ts.X = nmz.fit_transform(ts.X)\n",
    "\n",
    "# TODO write your code here\n",
    "# \n",
    "# write a for loop that tests the effect of evasion attacks on the poisoned classifiers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig = CFigure()\n",
    "for (sec_eval_data, number_poisoning_points) in zip(sec_evals, n_poisoning_samples):\n",
    "    fig.sp.plot_sec_eval(sec_eval_data, label=f\"p: {number_poisoning_points}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('secml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6829dbcfe73f7e6ba320fd39e7c4bddd23e92d1a15475ecfb0305a1647487c5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
